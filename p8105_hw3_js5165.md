p8105\_hw3\_js5165
================
Jingqi Song
October 13, 2018

Problem 1
---------

``` r
p1_brfss=brfss_smart2010 %>% 
  janitor::clean_names() %>%
  separate(locationdesc, into = c("state", "county"), sep = " - ") %>%
  filter(topic=='Overall Health') %>%
  mutate(response = forcats::fct_relevel(response, c("Excellent", "Very good", "Good", "Fair", "Poor"))) %>% 
  select(-locationabbr, year, state, county, response, everything())
```

``` r
#In 2002, which states were observed at 7 locations
p1_brfss%>%
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(count = n_distinct(county)) %>% 
  filter(count == 7)
```

    ## # A tibble: 3 x 2
    ##   state count
    ##   <chr> <int>
    ## 1 CT        7
    ## 2 FL        7
    ## 3 NC        7

In 2002, Connecticut, Florida and North Carolina were observed at 7 locations.

``` r
#plot that shows the number of locations in each state from 2002 to 2010
p1_brfss%>%
  group_by(state, year) %>% 
  summarize(location_num = n_distinct(county)) %>% 
  ggplot(aes(x = year, y = location_num, color = state)) + 
  geom_point() + 
  geom_line(alpha = .5) +
    labs(
    title = "Number of locations" ,
    x = "Year" ,
    y = "Number"
    ) +
  theme(legend.position = "right")
```

<img src="p8105_hw3_js5165_files/figure-markdown_github/Problem1.2-1.png" width="90%" />

Locations in most of the stateS are kept between 0-20 steadily across 2002-2010; two peak values of FL that are more than 40 are showed in 2007 and 2010.

``` r
#table for mean and standard deviation
p1_brfss_3 = p1_brfss %>%
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>%
  filter( year == 2002 | year == 2006 | year == 2010) %>%   
  filter(state == "NY") %>% 
  group_by(state, county) %>% 
  summarize(avg_excellent = mean(excellent, na.rm = TRUE),sd_excellent = sd(excellent, na.rm = TRUE)) %>% 
  knitr::kable(digits = 2)
p1_brfss_3
```

| state | county             |  avg\_excellent|  sd\_excellent|
|:------|:-------------------|---------------:|--------------:|
| NY    | Bronx County       |           17.60|            NaN|
| NY    | Erie County        |           17.20|            NaN|
| NY    | Kings County       |           20.37|           1.77|
| NY    | Monroe County      |           22.40|            NaN|
| NY    | Nassau County      |           24.93|           2.82|
| NY    | New York County    |           27.50|           1.54|
| NY    | Queens County      |           19.63|           1.36|
| NY    | Suffolk County     |           24.10|           3.28|
| NY    | Westchester County |           26.45|           0.64|

The average excellent proportion of all counties in NY are between 17.20-27.50, from which New York County is the hightes while Erie County is the lowest.

``` r
#five-panel plot 
p1_average = p1_brfss %>%
  spread(key = response, value = data_value) %>% 
  janitor::clean_names() %>%
  select(year, state, county, excellent:poor) %>% 
  group_by(state, year) %>% 
  summarize(avg_excellent = mean(excellent, na.rm = TRUE), avg_very_good = mean(very_good, na.rm = TRUE), avg_good = mean(good, na.rm = TRUE), avg_fair = mean(fair, na.rm = TRUE), avg_poor = mean(poor, na.rm = TRUE)) %>% 
  gather(key = "avg_type", value = "avg_proportion", avg_excellent:avg_poor) %>%
  mutate(avg_type = forcats::fct_relevel(avg_type, c("avg_excellent", "avg_very_good", "avg_good", "avg_fair", "avg_poor")))

ggplot(p1_average, aes(x = year, y = avg_proportion, color = state)) + 
  geom_point() + 
  geom_line(alpha = .5) +
  facet_grid(~ avg_type) +
  labs(
    title = "State-Level Average Response Proportion",
    x = "Year",
    y = "Average proportion"
  ) + 
  theme(legend.position = "right", axis.text.x = element_text(angle = 40))
```

<img src="p8105_hw3_js5165_files/figure-markdown_github/Problem1.4-1.png" width="90%" />

The ranking of average response proportion (from high to low) is "Very Good", "Good", "Excellent", "Fair", and "Poor"; the deviation of "Poor" is the lowest among the five responses.

Problem 2
---------

``` r
p2_instacart = instacart
```

The dataset has 1,384,617 observations and 15 variables. The variables include information (id and name) of departments, aisles and products, plus order information (eg. order id, number of order, order day). For example, 6 Bag of Organic Bananas (id: 13176) is ordered at 12am on the fifth day of the week (with no reorder) by the customer (id: 112108), and it has been 9 days since the last order. It is ordered from the Fresh Fruits aisle (id: 24) of department of Produce (id: 4). The order (id: 1) also includes some other products, such as Bulgarian Yogurt and Cucumber Kirby.

``` r
#which aisles are the most items ordered from
p2_instacart %>% 
  group_by(aisle) %>% 
  summarize(count = n()) %>% 
  arrange(-count)%>%
  head(5)
```

    ## # A tibble: 5 x 2
    ##   aisle                       count
    ##   <chr>                       <int>
    ## 1 fresh vegetables           150609
    ## 2 fresh fruits               150473
    ## 3 packaged vegetables fruits  78493
    ## 4 yogurt                      55240
    ## 5 packaged cheese             41699

There are in all 134 aisles; fresh vegetables and fresh fruits are the most items ordered from.

``` r
#plot that shows the number of items ordered in each aisle
p2_instacart %>% 
  group_by(aisle) %>% 
  summarize(count = n()) %>% 
  mutate(aisle = forcats::fct_reorder(aisle, count),
    aisle_grp = as.numeric(cut_number(count, 3))) %>% 
  ggplot(aes(x = aisle, y = count)) +
    geom_point() +
    facet_wrap(~ aisle_grp, nrow = 3, scales = "free") +
    theme(axis.text.x = element_text(size = 7, hjust = 1, angle = 40)) +
    labs(
      title = "Number of Items Ordered in Each Aisle",
      x = "Aisle Name",
      y = "Number of Items Ordered"
      )
```

<img src="p8105_hw3_js5165_files/figure-markdown_github/Problem2.2-1.png" width="90%" />

From the plot we can see that the number of items ordered in each aisle variates from less than 200 to more than 130,000. The highest are from fresh vegetables and fresh fruits; and there is a gap between the second highest (fresh fruits) and the third highest aisle(packaged vegetables fruits).

``` r
# the most popular item in “baking ingredients”, “dog food care”, and “packaged vegetables fruits”
p2_instacart %>%  
  filter(aisle == 'baking ingredients' | aisle == 'dog food care' | aisle == 'packaged vegetables fruits') %>% 
  group_by(aisle, product_name) %>% 
  summarize(count = n()) %>% 
  group_by(aisle) %>% 
  filter(min_rank(desc(count))< 2) 
```

    ## # A tibble: 3 x 3
    ## # Groups:   aisle [3]
    ##   aisle                    product_name                              count
    ##   <chr>                    <chr>                                     <int>
    ## 1 baking ingredients       Light Brown Sugar                           499
    ## 2 dog food care            Snack Sticks Chicken & Rice Recipe Dog T~    30
    ## 3 packaged vegetables fru~ Organic Baby Spinach                       9784

The most popular item in baking ingredients, dog food care and packaged vegetables fruits are Light Brown Suger, Snack Sticks Chicken & Rice Recipe Dog Treats, and Organic Baby Spinach respectively.

``` r
#table of the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered
p2_instacart %>%  
  filter(product_name == 'Pink Lady Apples' | product_name == 'Coffee Ice Cream') %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = round(mean(order_hour_of_day), 2)) %>% 
  spread(key = 'product_name', value = 'mean_hour') %>% 
  knitr::kable()
```

|  order\_dow|  Coffee Ice Cream|  Pink Lady Apples|
|-----------:|-----------------:|-----------------:|
|           0|             13.77|             13.44|
|           1|             14.32|             11.36|
|           2|             15.38|             11.70|
|           3|             15.32|             14.25|
|           4|             15.22|             11.55|
|           5|             12.26|             12.78|
|           6|             13.83|             11.94|

The value of the variable "order\_dow" (0-6) represents the first to the seventh day of the week. The range of mean hour time of Coffee Ice Cream is 1pm-4pm, which is generally later than the time Pink Lady Apples being ordered (12pm-3pm).

Problem 3
---------

``` r
p3_noaa = ny_noaa %>% 
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", 'day'), sep = "-") %>%
  mutate(
    tmax = as.numeric(tmax) / 10, 
    tmin = as.numeric(tmin) / 10,
    prcp = prcp / 10)

p3_missing = p3_noaa %>% 
  group_by(id) %>% 
  summarize(prcp_na = sum(is.na(prcp)),
            snow_na = sum(is.na(snow)),
            snwd_na = sum(is.na(snwd)),
            tmax_na = sum(is.na(tmax)),
            tmin_na = sum(is.na(tmin)))
p3_missing
```

    ## # A tibble: 747 x 6
    ##    id          prcp_na snow_na snwd_na tmax_na tmin_na
    ##    <chr>         <int>   <int>   <int>   <int>   <int>
    ##  1 US1NYAB0001      60     252     874    1157    1157
    ##  2 US1NYAB0006      19     283     703     852     852
    ##  3 US1NYAB0010     220     352     816     822     822
    ##  4 US1NYAB0016      98     152     212     214     214
    ##  5 US1NYAB0017     301     415     459     459     459
    ##  6 US1NYAB0021       2      37      72     365     365
    ##  7 US1NYAB0022     233     238     239     273     273
    ##  8 US1NYAB0023     144     238     289     365     365
    ##  9 US1NYAB0025      40      43      43     215     215
    ## 10 US1NYAL0002      57     245     409     549     549
    ## # ... with 737 more rows

The dataset has 2,595,176 observations and 9 variables. The variables include NY weather station id, the date of observation, precipitation (mm), snowfall (mm), snow depth (mm), and the maximum and minimum temperatures (tenths of degrees C). Also from the table of missing value, missing value may become a problem that needs to be considered when we are doing analysis using snow and temperature data.

``` r
#For snowfall, what are the most commonly observed values
p3_noaa %>% 
  count(snow) %>% 
  arrange(desc(n)) %>% 
  head(5)
```

    ## # A tibble: 5 x 2
    ##    snow       n
    ##   <int>   <int>
    ## 1     0 2008508
    ## 2    NA  381221
    ## 3    25   31022
    ## 4    13   23095
    ## 5    51   18274

For snowfall, 0 and 25 are the most commonly observed values.

``` r
#two-panel plot showing the average max temperature in January and in July
p3_avg = p3_noaa %>% 
  filter(month == '01' | month == '07') %>% 
  group_by(id, year, month) %>% 
  summarize(avg_tmax = round(mean(tmax, na.rm = TRUE), 2), avg_tmin = round(mean(tmin, na.rm = TRUE), 2)) %>% 
  na.omit() %>% 
  gather(key = "avg_type", value = "avg_temp", avg_tmax)

ggplot(p3_avg, aes(x = year, y = avg_temp)) + 
  geom_boxplot() + 
  facet_grid(~ month) +
  labs(
    title = "Average Maximum Temperature in January and July",
    x = "Year",
    y = "Average Maximum Temperature (C)"
  ) + 
  scale_y_continuous(breaks = c(-10, 0, 10, 20, 30)) +
  theme(legend.position = "bottom", axis.text.x = element_text(size = 6, angle = 40))
```

<img src="p8105_hw3_js5165_files/figure-markdown_github/Problem3.2-1.png" width="90%" />

There is no huge fluctuation of average maximum temperature in January and July across 1981 to 2010. A minor uptrend in January can be observed. Outliers are shown in January in 1982, 1993, 2004 and 2005, and July in 1988, 2004 and 2007.

``` r
#(i) tmax vs tmin for the full dataset
p3_noaa %>% 
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_hex() +
  labs(
    title = "Maximum Temperature vs Minimum Temperature",
    x = "Maximum Temperature (C)",
    y = "Minimum Temperature (C)"
  )
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_binhex).

<img src="p8105_hw3_js5165_files/figure-markdown_github/Problem3.3-1.png" width="90%" />

The lighter color of the plot represents greater number of observations. We can see from the plot that there might be a linear relationship between maximum and minimum temperature.

``` r
#(ii) plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year
p3_noaa %>%
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = snow, y = year)) + 
  geom_density_ridges(scale = .85) +
  labs(
    title = "Snowfall 0-100 mm Each Year",
    x = "Snow Fall (mm)", 
    y = "Year")
```

    ## Picking joint bandwidth of 3.76

<img src="p8105_hw3_js5165_files/figure-markdown_github/Problem3.4-1.png" width="90%" />

There is no huge fluctuation of snowfall across 1981 to 2010.
